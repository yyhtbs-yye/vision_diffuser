boat:
  type: E2ELDM

  model:
    path: 'diffusers.models.unets'    # Path to import the model class
    name: 'UNet2DModel'               # Name of the model class to use
    config:
      in_channels: 4                  # VAE Latent channels
      out_channels: 4                 # VAE Latent channels
      sample_size: [64, 64]           # VAE Latent feature size
      down_block_types: ['DownBlock2D', 'DownBlock2D', 'DownBlock2D', 'AttnDownBlock2D']
      up_block_types: ['AttnUpBlock2D', 'UpBlock2D', 'UpBlock2D', 'UpBlock2D']
      block_out_channels: [128, 256, 512, 512]
      layers_per_block: 2
      attention_head_dim: 8
      norm_num_groups: 32
      norm_eps: 0.00001
  
  encoder:
    path: 'vision.networks.autoencoder_wrappers'   # Path to import the model class
    name: 'AutoencoderKLWrapper'              # Name of the model class to use
    config:
      down_block_types: [DownEncoderBlock2D, DownEncoderBlock2D, DownEncoderBlock2D, DownEncoderBlock2D]
      up_block_types: [UpDecoderBlock2D, UpDecoderBlock2D, UpDecoderBlock2D, UpDecoderBlock2D]
      block_out_channels: [64, 128, 256, 256]
      latent_channels: 4
      layers_per_block: 2
      act_fn: silu
      downsample_padding: 0
      mid_block_add_attention: true
      scaling_factor: 0.1828
      force_upcast: true
      sample_size: 64

  # Scheduler configuration
  scheduler:
    path: 'vision.schedulers.scheduling_ddim'           # Path to import the model class
    name: 'DDIMScheduler'         # Name of the model class to use
    config:
      beta_start: 0.0001
      beta_end: 0.02
      beta_schedule: "linear"
      steps_offset: 1
      clip_sample: false
      set_alpha_to_one: false
      prediction_type: "epsilon"
      num_train_timesteps: 1000

  solver:
    path: 'vision.solvers.sampling_ddim'           # Path to import the model class
    name: 'DDIMSampler'         # Name of the model class to use
    config:
      beta_start: 0.0001
      beta_end: 0.02
      beta_schedule: "linear"
      steps_offset: 1
      clip_sample: false
      set_alpha_to_one: false
      prediction_type: "epsilon"
      num_train_timesteps: 1000
      eta: 0.0                   # Parameter controlling noise level in sampling
      num_inference_steps: 50    # Default sampling steps for inference


# Validation configuration
validation:
  use_visualizer: true
  num_vis_samples: 4          # Number of samples to visualize during validation
  metrics:
    use_fid: false
    custom_metrics: {}        # Optional custom metrics can be defined here

# Training configuration
train:
  max_epochs: 100
  val_check_interval: 200
  loss:
    noise_estimation_weight: 0.4
    image_denoise_weight: 0.3
    image_decode_weight: 0.3
  # Optimizer configuration
  optimizer:
    learning_rate: 0.0001
    betas: [0.9, 0.999]
    weight_decay: 0.0
    use_ema: true
    ema_decay: 0.999
    ema_start: 1000
    noise_offset_weight: 0.0    # Parameter for noise offset (disabled in  code)

# Data configuration
data:
  train_dataloader:
    dataset:
      path: 'vision.torch_datasets.basic_image_dataset'           # Path to import the model class
      name: 'BasicImageDataset'         # Name of the model class to use
      config:
        folder_paths:
          gt: data/ffhq/ffhq_imgs/ffhq_64
        data_prefix:
          gt: ''
        pipeline:
          - type: LoadImageFromFile
            keys: [gt]
          - type: Normalize
            keys: [gt]
            mean: [0.5, 0.5, 0.5]
            std: [0.5, 0.5, 0.5]
    batch_size: 32
    num_workers: 32
    persistent_workers: true
    sampler:
      type: DefaultSampler
      shuffle: true

  # Validation dataloader configuration
  val_dataloader:
    dataset:
      path: 'vision.torch_datasets.basic_image_dataset'           # Path to import the model class
      name: 'BasicImageDataset'         # Name of the model class to use
      config:
        folder_paths:
          gt: data/celeba/subsets/celeba_64
        data_prefix:
          gt: ''
        pipeline:
          - type: LoadImageFromFile
            keys: [gt]
          - type: Normalize
            keys: [gt]
            mean: [0.5, 0.5, 0.5]
            std: [0.5, 0.5, 0.5]
    batch_size: 64
    num_workers: 32
    persistent_workers: true
    sampler:
      type: DefaultSampler
      shuffle: true

# Logging configuration
logging:
  log_dir: work_dirs/e2e_ldm
  experiment_name: e2e_ldm_64
  log_every_n_steps: 50

# Checkpoint configuration
checkpoint:
  save_best_metric: 'val/noise_mse'  # Changed from latent_mse to img_mse
  save_best_mode: 'min'            # 'min' for loss metrics
  save_top_k: 3                    # Number of best checkpoints to keep
  save_last: true                  # Whether to save the last checkpoint

# Visualization configuration, not implemented yet (now manually)
# visualization:
#   vis_backends:
#     - type: LocalVisBackend
#   visualizer:
#     type: ConcatImageVisualizer
#     vis_backends: ${visualization.vis_backends}
#     fn_key: gt_path
#     img_keys: [gt_img, pred_img]
#     bgr2rgb: true
#   custom_hooks:
#     - type: BasicVisualizationHook
#       interval: 1